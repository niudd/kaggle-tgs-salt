{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lovasz_losses as L\n",
    "from metrics import iou_pytorch\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    # read numpy format data\n",
    "    with open('../data/processed/dataset_%d.pkl'%SEED, 'rb') as f:\n",
    "        ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = pickle.load(f)\n",
    "    if debug:\n",
    "        x_train, y_train = x_train[:400], y_train[:400]\n",
    "        x_valid, y_valid = x_valid[:50], y_valid[:50]\n",
    "        \n",
    "    # make pytorch.data.Dataset\n",
    "    train_ds = TgsDataSet(x_train, y_train)\n",
    "    val_ds = TgsDataSet(x_valid, y_valid)\n",
    "    \n",
    "    train_dl = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        #sampler=StratifiedSampler(),\n",
    "        num_workers=NUM_WORKERS,\n",
    "    )\n",
    "    \n",
    "    val_dl = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        #sampler=StratifiedSampler(),\n",
    "        num_workers=NUM_WORKERS,\n",
    "    )\n",
    "    \n",
    "    return train_dl, val_dl\n",
    "\n",
    "from dataset import TgsDataSet\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "SEED = 1234\n",
    "debug = True\n",
    "BATCH_SIZE = 16#32\n",
    "NUM_WORKERS = 20\n",
    "train_dl, val_dl = prepare_data()\n",
    "\n",
    "\n",
    "for i,(x,y) in enumerate(train_dl):\n",
    "    x = x.to(device=device, dtype=torch.float)\n",
    "    y = y.to(device=device, dtype=torch.float)\n",
    "    # for classify zero mask modelling, 1: zero 0: nonzero\n",
    "    y = y.reshape(-1, 256*256).sum(dim=1, keepdim=True)==0\n",
    "    if i==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiment with net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lovasz_losses as L\n",
    "#from metrics import iou_pytorch\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "\n",
    "class ConvBn2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(3,3), stride=(1,1), padding=(1,1)):\n",
    "        super(ConvBn2d, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        #self.dropout = nn.Dropout2d(p=0.1, inplace=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        #self.bn = SynchronizedBatchNorm2d(out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.conv(z)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 =  ConvBn2d(in_channels,  channels, kernel_size=3, padding=1)\n",
    "        self.conv2 =  ConvBn2d(channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.spa_cha_gate = SCSE(out_channels)\n",
    "\n",
    "    def forward(self, x, e=None):\n",
    "        x = F.upsample(x, scale_factor=2, mode='bilinear', align_corners=True)#False\n",
    "        if e is not None:\n",
    "            x = torch.cat([x, e], 1)\n",
    "        x = F.relu(self.conv1(x),inplace=True)\n",
    "        x = F.relu(self.conv2(x),inplace=True)\n",
    "        x = self.spa_cha_gate(x)\n",
    "        return x\n",
    "\n",
    "class SCSE(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super(SCSE, self).__init__()\n",
    "        self.spatial_gate = SpatialGate2d(in_ch, 16)#16\n",
    "        self.channel_gate = ChannelGate2d(in_ch)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        g1 = self.spatial_gate(x)\n",
    "        g2 = self.channel_gate(x)\n",
    "        x = g1 + g2 #x = g1*x + g2*x\n",
    "        return x\n",
    "\n",
    "class SpatialGate2d(nn.Module):\n",
    "    def __init__(self, in_ch, r=16):\n",
    "        super(SpatialGate2d, self).__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(in_ch, in_ch//r)\n",
    "        self.linear_2 = nn.Linear(in_ch//r, in_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_x = x\n",
    "\n",
    "        x = x.view(*(x.shape[:-2]),-1).mean(-1)\n",
    "        x = F.relu(self.linear_1(x), inplace=True)\n",
    "        x = self.linear_2(x)\n",
    "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        x = input_x * x\n",
    "\n",
    "        return x\n",
    "\n",
    "class ChannelGate2d(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super(ChannelGate2d, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_ch, 1, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_x = x\n",
    "\n",
    "        x = self.conv(x)\n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        x = input_x * x\n",
    "\n",
    "        return x\n",
    "\n",
    "class UNetResNet34(nn.Module):\n",
    "    # PyTorch U-Net model using ResNet(34, 50 , 101 or 152) encoder.\n",
    "    def load_pretrain(self, pretrain_file):\n",
    "        self.encoder.load_state_dict(torch.load(pretrain_file, map_location=lambda storage, loc: storage))\n",
    "\n",
    "    def __init__(self, pretrained=True, debug=False):\n",
    "        super().__init__()\n",
    "        self.resnet = torchvision.models.resnet34(pretrained=pretrained)\n",
    "        self.debug = debug\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            self.resnet.conv1,\n",
    "            self.resnet.bn1,\n",
    "            self.resnet.relu,\n",
    "            #self.resnet.maxpool,\n",
    "        )# 64\n",
    "        self.encoder2 = nn.Sequential(self.resnet.layer1, SCSE(64))\n",
    "        self.encoder3 = nn.Sequential(self.resnet.layer2, SCSE(128))\n",
    "        self.encoder4 = nn.Sequential(self.resnet.layer3, SCSE(256))\n",
    "        self.encoder5 = nn.Sequential(self.resnet.layer4, SCSE(512))\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(51200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #batch_size,C,H,W = x.shape\n",
    "\n",
    "        mean=[0.485, 0.456, 0.406]\n",
    "        std =[0.229, 0.224, 0.225]\n",
    "        x = torch.cat([\n",
    "            (x-mean[0])/std[0],\n",
    "            (x-mean[1])/std[1],\n",
    "            (x-mean[2])/std[2],\n",
    "        ],1)\n",
    "        #x = add_depth_channels(x)\n",
    "        \n",
    "        if self.debug:\n",
    "            print('input: ', x.size())\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        if self.debug:\n",
    "            print('e1',x.size())\n",
    "        e2 = self.encoder2(x)\n",
    "        if self.debug:\n",
    "            print('e2',e2.size())\n",
    "        e3 = self.encoder3(e2)\n",
    "        if self.debug:\n",
    "            print('e3',e3.size())\n",
    "        e4 = self.encoder4(e3)\n",
    "        if self.debug:\n",
    "            print('e4',e4.size())\n",
    "        e5 = self.encoder5(e4)\n",
    "        if self.debug:\n",
    "            print('e5',e5.size())\n",
    "\n",
    "        f = self.avgpool(e5)\n",
    "        if self.debug:\n",
    "            print('avgpool: ',f.size())\n",
    "        f = F.dropout(f, p=0.40)#training=self.training\n",
    "        \n",
    "        f = f.view(f.size(0), -1)\n",
    "        logit = self.fc(f)\n",
    "        if self.debug:\n",
    "            print('fc: ', logit.size())\n",
    "        return logit\n",
    "\n",
    "        ##-----------------------------------------------------------------\n",
    "\n",
    "    def criterion(self, logit, truth):\n",
    "        \"\"\"Define the (customized) loss function here.\"\"\"\n",
    "        loss = L.binary_xloss(logit, truth, ignore=255)\n",
    "        return loss\n",
    "\n",
    "    def metric(self, logit, truth):\n",
    "        \"\"\"Define metrics for evaluation especially for early stoppping.\"\"\"\n",
    "        auc = roc_auc_score(truth.detach(), logit.detach())\n",
    "        #tn, fp, fn, tp = confusion_matrix(truth.detach(), logit.detach()).ravel()\n",
    "        return auc#, [tn, fp, fn, tp]\n",
    "\n",
    "    def set_mode(self, mode):\n",
    "        self.mode = mode\n",
    "        if mode in ['eval', 'valid', 'test']:\n",
    "            self.eval()\n",
    "        elif mode in ['train']:\n",
    "            self.train()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "def predict_proba(net, test_dl, device):\n",
    "    y_pred = None\n",
    "    net.set_mode('test')\n",
    "    with torch.no_grad():\n",
    "        for i, (input_data, truth) in enumerate(test_dl):\n",
    "            #if i > 10:\n",
    "            #    break\n",
    "            input_data, truth = input_data.to(device=device, dtype=torch.float), truth.to(device=device, dtype=torch.float)\n",
    "            logit = net(input_data).cpu().numpy()\n",
    "            if y_pred is None:\n",
    "                y_pred = logit\n",
    "            else:\n",
    "                y_pred = np.concatenate([y_pred, logit], axis=0)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def add_depth_channels(image_tensor):\n",
    "    _, _, h, w = image_tensor.size()\n",
    "    x_depth_channel = torch.ones(image_tensor.size(), dtype=torch.float64, device=image_tensor.device)\n",
    "    for row, const in enumerate(np.linspace(0, 1, h)):\n",
    "        x_depth_channel[:, 0, row, :] = const\n",
    "    x_depth_channel = x_depth_channel.float()\n",
    "    x_depth_channel_mul = image_tensor * x_depth_channel\n",
    "    image_tensor = torch.cat([image_tensor, x_depth_channel, x_depth_channel_mul], 1)\n",
    "    return image_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNetResNet34(debug=True).cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  torch.Size([16, 3, 256, 256])\n",
      "e1 torch.Size([16, 64, 128, 128])\n",
      "e2 torch.Size([16, 64, 128, 128])\n",
      "e3 torch.Size([16, 128, 64, 64])\n",
      "e4 torch.Size([16, 256, 32, 32])\n",
      "e5 torch.Size([16, 512, 16, 16])\n",
      "avgpool:  torch.Size([16, 512, 10, 10])\n",
      "fc:  torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "# new decoder version\n",
    "output = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6579, device='cuda:1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.criterion(output, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:  21894377\n",
      "Encoder parameters:  21797672\n",
      "Center parameters:  0\n",
      "Decoder parameters:  0\n"
     ]
    }
   ],
   "source": [
    "#layer_name = 'decoder'\n",
    "#layer_name = 'resnet'\n",
    "#layer_name = 'center'\n",
    "#layer_name = '.'\n",
    "\n",
    "#layer_name = '.'\n",
    "\n",
    "print('Total parameters: ', sum(p[1].numel() for p in net.named_parameters() \\\n",
    "                                if '.' in p[0]))\n",
    "\n",
    "#print('Trainable parameters: ', sum(p[1].numel() for p in net.named_parameters() \\\n",
    "#                                    if p[1].requires_grad and layer_name in p[0]))\n",
    "\n",
    "print('Encoder parameters: ', sum(p[1].numel() for p in net.named_parameters() \\\n",
    "                                if 'resnet' in p[0]))\n",
    "\n",
    "print('Center parameters: ', sum(p[1].numel() for p in net.named_parameters() \\\n",
    "                                if 'center' in p[0]))\n",
    "\n",
    "print('Decoder parameters: ', sum(p[1].numel() for p in net.named_parameters() \\\n",
    "                                if 'decoder' in p[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet.conv1.weight\n",
      "resnet.bn1.weight\n",
      "resnet.bn1.bias\n",
      "resnet.layer1.0.conv1.weight\n",
      "resnet.layer1.0.bn1.weight\n",
      "resnet.layer1.0.bn1.bias\n",
      "resnet.layer1.0.conv2.weight\n",
      "resnet.layer1.0.bn2.weight\n",
      "resnet.layer1.0.bn2.bias\n",
      "resnet.layer1.1.conv1.weight\n",
      "resnet.layer1.1.bn1.weight\n",
      "resnet.layer1.1.bn1.bias\n",
      "resnet.layer1.1.conv2.weight\n",
      "resnet.layer1.1.bn2.weight\n",
      "resnet.layer1.1.bn2.bias\n",
      "resnet.layer1.2.conv1.weight\n",
      "resnet.layer1.2.bn1.weight\n",
      "resnet.layer1.2.bn1.bias\n",
      "resnet.layer1.2.conv2.weight\n",
      "resnet.layer1.2.bn2.weight\n",
      "resnet.layer1.2.bn2.bias\n",
      "resnet.layer2.0.conv1.weight\n",
      "resnet.layer2.0.bn1.weight\n",
      "resnet.layer2.0.bn1.bias\n",
      "resnet.layer2.0.conv2.weight\n",
      "resnet.layer2.0.bn2.weight\n",
      "resnet.layer2.0.bn2.bias\n",
      "resnet.layer2.0.downsample.0.weight\n",
      "resnet.layer2.0.downsample.1.weight\n",
      "resnet.layer2.0.downsample.1.bias\n",
      "resnet.layer2.1.conv1.weight\n",
      "resnet.layer2.1.bn1.weight\n",
      "resnet.layer2.1.bn1.bias\n",
      "resnet.layer2.1.conv2.weight\n",
      "resnet.layer2.1.bn2.weight\n",
      "resnet.layer2.1.bn2.bias\n",
      "resnet.layer2.2.conv1.weight\n",
      "resnet.layer2.2.bn1.weight\n",
      "resnet.layer2.2.bn1.bias\n",
      "resnet.layer2.2.conv2.weight\n",
      "resnet.layer2.2.bn2.weight\n",
      "resnet.layer2.2.bn2.bias\n",
      "resnet.layer2.3.conv1.weight\n",
      "resnet.layer2.3.bn1.weight\n",
      "resnet.layer2.3.bn1.bias\n",
      "resnet.layer2.3.conv2.weight\n",
      "resnet.layer2.3.bn2.weight\n",
      "resnet.layer2.3.bn2.bias\n",
      "resnet.layer3.0.conv1.weight\n",
      "resnet.layer3.0.bn1.weight\n",
      "resnet.layer3.0.bn1.bias\n",
      "resnet.layer3.0.conv2.weight\n",
      "resnet.layer3.0.bn2.weight\n",
      "resnet.layer3.0.bn2.bias\n",
      "resnet.layer3.0.downsample.0.weight\n",
      "resnet.layer3.0.downsample.1.weight\n",
      "resnet.layer3.0.downsample.1.bias\n",
      "resnet.layer3.1.conv1.weight\n",
      "resnet.layer3.1.bn1.weight\n",
      "resnet.layer3.1.bn1.bias\n",
      "resnet.layer3.1.conv2.weight\n",
      "resnet.layer3.1.bn2.weight\n",
      "resnet.layer3.1.bn2.bias\n",
      "resnet.layer3.2.conv1.weight\n",
      "resnet.layer3.2.bn1.weight\n",
      "resnet.layer3.2.bn1.bias\n",
      "resnet.layer3.2.conv2.weight\n",
      "resnet.layer3.2.bn2.weight\n",
      "resnet.layer3.2.bn2.bias\n",
      "resnet.layer3.3.conv1.weight\n",
      "resnet.layer3.3.bn1.weight\n",
      "resnet.layer3.3.bn1.bias\n",
      "resnet.layer3.3.conv2.weight\n",
      "resnet.layer3.3.bn2.weight\n",
      "resnet.layer3.3.bn2.bias\n",
      "resnet.layer3.4.conv1.weight\n",
      "resnet.layer3.4.bn1.weight\n",
      "resnet.layer3.4.bn1.bias\n",
      "resnet.layer3.4.conv2.weight\n",
      "resnet.layer3.4.bn2.weight\n",
      "resnet.layer3.4.bn2.bias\n",
      "resnet.layer3.5.conv1.weight\n",
      "resnet.layer3.5.bn1.weight\n",
      "resnet.layer3.5.bn1.bias\n",
      "resnet.layer3.5.conv2.weight\n",
      "resnet.layer3.5.bn2.weight\n",
      "resnet.layer3.5.bn2.bias\n",
      "resnet.layer4.0.conv1.weight\n",
      "resnet.layer4.0.bn1.weight\n",
      "resnet.layer4.0.bn1.bias\n",
      "resnet.layer4.0.conv2.weight\n",
      "resnet.layer4.0.bn2.weight\n",
      "resnet.layer4.0.bn2.bias\n",
      "resnet.layer4.0.downsample.0.weight\n",
      "resnet.layer4.0.downsample.1.weight\n",
      "resnet.layer4.0.downsample.1.bias\n",
      "resnet.layer4.1.conv1.weight\n",
      "resnet.layer4.1.bn1.weight\n",
      "resnet.layer4.1.bn1.bias\n",
      "resnet.layer4.1.conv2.weight\n",
      "resnet.layer4.1.bn2.weight\n",
      "resnet.layer4.1.bn2.bias\n",
      "resnet.layer4.2.conv1.weight\n",
      "resnet.layer4.2.bn1.weight\n",
      "resnet.layer4.2.bn1.bias\n",
      "resnet.layer4.2.conv2.weight\n",
      "resnet.layer4.2.bn2.weight\n",
      "resnet.layer4.2.bn2.bias\n",
      "resnet.fc.weight\n",
      "resnet.fc.bias\n",
      "center.0.conv.weight\n",
      "center.0.bn.weight\n",
      "center.0.bn.bias\n",
      "center.2.conv.weight\n",
      "center.2.bn.weight\n",
      "center.2.bn.bias\n",
      "decoder5.conv1.conv.weight\n",
      "decoder5.conv1.bn.weight\n",
      "decoder5.conv1.bn.bias\n",
      "decoder5.conv2.conv.weight\n",
      "decoder5.conv2.bn.weight\n",
      "decoder5.conv2.bn.bias\n",
      "decoder5.spatial_gate.linear_1.weight\n",
      "decoder5.spatial_gate.linear_1.bias\n",
      "decoder5.spatial_gate.linear_2.weight\n",
      "decoder5.spatial_gate.linear_2.bias\n",
      "decoder5.channel_gate.conv.weight\n",
      "decoder5.channel_gate.conv.bias\n",
      "decoder4.conv1.conv.weight\n",
      "decoder4.conv1.bn.weight\n",
      "decoder4.conv1.bn.bias\n",
      "decoder4.conv2.conv.weight\n",
      "decoder4.conv2.bn.weight\n",
      "decoder4.conv2.bn.bias\n",
      "decoder4.spatial_gate.linear_1.weight\n",
      "decoder4.spatial_gate.linear_1.bias\n",
      "decoder4.spatial_gate.linear_2.weight\n",
      "decoder4.spatial_gate.linear_2.bias\n",
      "decoder4.channel_gate.conv.weight\n",
      "decoder4.channel_gate.conv.bias\n",
      "decoder3.conv1.conv.weight\n",
      "decoder3.conv1.bn.weight\n",
      "decoder3.conv1.bn.bias\n",
      "decoder3.conv2.conv.weight\n",
      "decoder3.conv2.bn.weight\n",
      "decoder3.conv2.bn.bias\n",
      "decoder3.spatial_gate.linear_1.weight\n",
      "decoder3.spatial_gate.linear_1.bias\n",
      "decoder3.spatial_gate.linear_2.weight\n",
      "decoder3.spatial_gate.linear_2.bias\n",
      "decoder3.channel_gate.conv.weight\n",
      "decoder3.channel_gate.conv.bias\n",
      "decoder2.conv1.conv.weight\n",
      "decoder2.conv1.bn.weight\n",
      "decoder2.conv1.bn.bias\n",
      "decoder2.conv2.conv.weight\n",
      "decoder2.conv2.bn.weight\n",
      "decoder2.conv2.bn.bias\n",
      "decoder2.spatial_gate.linear_1.weight\n",
      "decoder2.spatial_gate.linear_1.bias\n",
      "decoder2.spatial_gate.linear_2.weight\n",
      "decoder2.spatial_gate.linear_2.bias\n",
      "decoder2.channel_gate.conv.weight\n",
      "decoder2.channel_gate.conv.bias\n",
      "decoder1.conv1.conv.weight\n",
      "decoder1.conv1.bn.weight\n",
      "decoder1.conv1.bn.bias\n",
      "decoder1.conv2.conv.weight\n",
      "decoder1.conv2.bn.weight\n",
      "decoder1.conv2.bn.bias\n",
      "decoder1.spatial_gate.linear_1.weight\n",
      "decoder1.spatial_gate.linear_1.bias\n",
      "decoder1.spatial_gate.linear_2.weight\n",
      "decoder1.spatial_gate.linear_2.bias\n",
      "decoder1.channel_gate.conv.weight\n",
      "decoder1.channel_gate.conv.bias\n",
      "logit.0.weight\n",
      "logit.0.bias\n",
      "logit.2.weight\n",
      "logit.2.bias\n"
     ]
    }
   ],
   "source": [
    "for i,p in enumerate(net.named_parameters()):\n",
    "    print(p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [p for p in net.named_parameters() if layer_name in p[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 7, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = layers[0]\n",
    "layer[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(x,y) in enumerate(train_dl):\n",
    "    x = x.to(device=device, dtype=torch.float)\n",
    "    y = y.to(device=device, dtype=torch.float)\n",
    "    if i==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## freeze layers parameters\n",
    "\n",
    "for param in net.named_parameters():\n",
    "    #print(param[0][:8])\n",
    "    if param[0][:8] in ['decoder5']:#'decoder5', 'decoder4', 'decoder3', 'decoder2'\n",
    "        #print('no')\n",
    "        param[1].requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## different version : nn.AdaptiveAvgPool2d layer\n",
    "class SCSE(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SCSE, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.channel_excitation = nn.Sequential(nn.Linear(channel, int(channel//reduction)),\n",
    "                                                nn.ReLU(inplace=True),\n",
    "                                                nn.Linear(int(channel//reduction), channel),\n",
    "                                                nn.Sigmoid())\n",
    "\n",
    "        self.spatial_se = nn.Sequential(nn.Conv2d(channel, 1, kernel_size=1,\n",
    "                                                  stride=1, padding=0, bias=False),\n",
    "                                        nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        bahs, chs, _, _ = x.size()\n",
    "\n",
    "        # Returns a new tensor with the same data as the self tensor but of a different size.\n",
    "        chn_se = self.avg_pool(x).view(bahs, chs)\n",
    "        chn_se = self.channel_excitation(chn_se).view(bahs, chs, 1, 1)\n",
    "        chn_se = torch.mul(x, chn_se)\n",
    "\n",
    "        spa_se = self.spatial_se(x)\n",
    "        spa_se = torch.mul(x, spa_se)\n",
    "        return torch.add(chn_se, 1, spa_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## neptune's open solutions\n",
    "\n",
    "class ConvBnRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "                                  nn.BatchNorm2d(out_channels),\n",
    "                                  nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class DecoderBlockV2(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n",
    "        super(DecoderBlockV2, self).__init__()\n",
    "        self.is_deconv = is_deconv\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            ConvBnRelu(in_channels, middle_channels),\n",
    "            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            ConvBnRelu(in_channels, out_channels),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.is_deconv:\n",
    "            x = self.deconv(x)\n",
    "        else:\n",
    "            x = self.upsample(x)\n",
    "        return x\n",
    "\n",
    "class SaltLinkNet(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_2d=0.2, pretrained=False, is_deconv=False):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_2d = dropout_2d\n",
    "\n",
    "        self.encoder = torchvision.models.resnet34(pretrained=pretrained)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.input_adjust = nn.Sequential(self.encoder.conv1,\n",
    "                                          self.encoder.bn1,\n",
    "                                          self.encoder.relu)\n",
    "\n",
    "        self.conv1_1 = list(self.encoder.layer1.children())[1]\n",
    "        self.conv1_2 = list(self.encoder.layer1.children())[2]\n",
    "\n",
    "        self.conv2_0 = list(self.encoder.layer2.children())[0]\n",
    "        self.conv2_1 = list(self.encoder.layer2.children())[1]\n",
    "        self.conv2_2 = list(self.encoder.layer2.children())[2]\n",
    "        self.conv2_3 = list(self.encoder.layer2.children())[3]\n",
    "\n",
    "        self.dec2 = DecoderBlockV2(128, 256, 256, is_deconv=is_deconv)\n",
    "        self.dec1 = DecoderBlockV2(256 + 64, 512, 256, is_deconv=is_deconv)\n",
    "        self.final = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean=[0.485, 0.456, 0.406]\n",
    "        std =[0.229, 0.224, 0.225]\n",
    "        x = torch.cat([\n",
    "            (x-mean[0])/std[0],\n",
    "            (x-mean[1])/std[1],\n",
    "            (x-mean[2])/std[2],\n",
    "        ],1)\n",
    "        print('input shape: ', x.size())\n",
    "        \n",
    "        input_adjust = self.input_adjust(x)\n",
    "        conv1_1 = self.conv1_1(input_adjust)\n",
    "        conv1_2 = self.conv1_2(conv1_1)\n",
    "        conv2_0 = self.conv2_0(conv1_2)\n",
    "        conv2_1 = self.conv2_1(conv2_0)\n",
    "        conv2_2 = self.conv2_2(conv2_1)\n",
    "        conv2_3 = self.conv2_3(conv2_2)\n",
    "\n",
    "        conv1_sum = conv1_1 + conv1_2\n",
    "        conv2_sum = conv2_0 + conv2_1 + conv2_2 + conv2_3\n",
    "\n",
    "        dec2 = self.dec2(conv2_sum)\n",
    "        dec1 = self.dec1(torch.cat([dec2, conv1_sum], 1))\n",
    "\n",
    "        return self.final(F.dropout2d(dec1, p=self.dropout_2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
